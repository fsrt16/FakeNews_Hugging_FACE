{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport unicodedata\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout, Input, Reshape\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import regularizers\nfrom transformers import *\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing \nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-28T07:02:53.657619Z","iopub.execute_input":"2021-08-28T07:02:53.657930Z","iopub.status.idle":"2021-08-28T07:03:02.157057Z","shell.execute_reply.started":"2021-08-28T07:02:53.657901Z","shell.execute_reply":"2021-08-28T07:03:02.156098Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n  '\"sox\" backend is being deprecated. '\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install numpy requests nlpaug\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.word as nlpaw\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:02.158697Z","iopub.execute_input":"2021-08-28T07:03:02.159047Z","iopub.status.idle":"2021-08-28T07:03:11.532478Z","shell.execute_reply.started":"2021-08-28T07:03:02.159009Z","shell.execute_reply":"2021-08-28T07:03:11.531648Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.25.1)\nCollecting nlpaug\n  Downloading nlpaug-1.1.7-py3-none-any.whl (405 kB)\n\u001b[K     |████████████████████████████████| 405 kB 596 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.26.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.10)\nInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.7\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers==3.0.0","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:11.536068Z","iopub.execute_input":"2021-08-28T07:03:11.536331Z","iopub.status.idle":"2021-08-28T07:03:20.742535Z","shell.execute_reply.started":"2021-08-28T07:03:11.536304Z","shell.execute_reply":"2021-08-28T07:03:20.741460Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting transformers==3.0.0\n  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n\u001b[K     |████████████████████████████████| 754 kB 606 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (2.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (2021.4.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (1.19.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (20.9)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (0.1.96)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (3.0.12)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (0.0.45)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.0) (4.61.1)\nCollecting tokenizers==0.8.0-rc4\n  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 13.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.0) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.0) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.0) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.0) (1.26.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.0) (4.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.0) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.0) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.0) (1.15.0)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.10.3\n    Uninstalling tokenizers-0.10.3:\n      Successfully uninstalled tokenizers-0.10.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.6.1\n    Uninstalling transformers-4.6.1:\n      Successfully uninstalled transformers-4.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.5.0 requires transformers<4.7,>=4.1, but you have transformers 3.0.0 which is incompatible.\u001b[0m\nSuccessfully installed tokenizers-0.8.0rc4 transformers-3.0.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"dfaug.to_csv('df_aug.csv',index=False)\ndfaug.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:04:20.546811Z","iopub.execute_input":"2021-08-28T05:04:20.547103Z","iopub.status.idle":"2021-08-28T05:04:20.856093Z","shell.execute_reply.started":"2021-08-28T05:04:20.547072Z","shell.execute_reply":"2021-08-28T05:04:20.855298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfaug.groupby('Pred').count()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:04:20.857159Z","iopub.execute_input":"2021-08-28T05:04:20.857528Z","iopub.status.idle":"2021-08-28T05:04:20.877961Z","shell.execute_reply.started":"2021-08-28T05:04:20.857501Z","shell.execute_reply":"2021-08-28T05:04:20.877224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfaug = pd.read_csv('../input/newsdatasets/df_aug.csv')\nmax_len = 30","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:20.746492Z","iopub.execute_input":"2021-08-28T07:03:20.746795Z","iopub.status.idle":"2021-08-28T07:03:21.018117Z","shell.execute_reply.started":"2021-08-28T07:03:20.746765Z","shell.execute_reply":"2021-08-28T07:03:21.017220Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndfaug = dfaug.groupby('Pred').apply(lambda x: x.sample(4000)).reset_index(drop=True)\ndfaug['Sent'] = dfaug['Sent'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:21.019461Z","iopub.execute_input":"2021-08-28T07:03:21.019838Z","iopub.status.idle":"2021-08-28T07:03:21.070583Z","shell.execute_reply.started":"2021-08-28T07:03:21.019800Z","shell.execute_reply":"2021-08-28T07:03:21.069803Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\nle = preprocessing.LabelEncoder()\ndfaug = dfaug.sample(frac = 1)\nX = dfaug['Sent'].tolist()\nY = np.array(dfaug.iloc[:]['Pred'])\nle.fit(Y)\nY = le.transform(Y)\nnum_classes=  len(le.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:21.071827Z","iopub.execute_input":"2021-08-28T07:03:21.072174Z","iopub.status.idle":"2021-08-28T07:03:21.141093Z","shell.execute_reply.started":"2021-08-28T07:03:21.072139Z","shell.execute_reply":"2021-08-28T07:03:21.140343Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"le.classes_","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:21.142887Z","iopub.execute_input":"2021-08-28T07:03:21.143223Z","iopub.status.idle":"2021-08-28T07:03:21.151031Z","shell.execute_reply.started":"2021-08-28T07:03:21.143187Z","shell.execute_reply":"2021-08-28T07:03:21.150032Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire',\n       'true'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Transfer Learning","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom transformers import BertTokenizer, TFBertModel, TFDistilBertModel,DistilBertTokenizer , RobertaTokenizer, TFRobertaModel\n#dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n#dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n#robert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n#robert_model = TFRobertaModel.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:21.199217Z","iopub.execute_input":"2021-08-28T07:03:21.199461Z","iopub.status.idle":"2021-08-28T07:03:53.796585Z","shell.execute_reply.started":"2021-08-28T07:03:21.199437Z","shell.execute_reply":"2021-08-28T07:03:53.795647Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f4e46dbf754757a5e363dbc8ce1b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb2a2ea01fe40e3b23458b67f975a99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5b8078e0714b6f9e3773dcf500db2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6db9dcfc6d4cc79939d0d32856dc40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3700e700d3435382ac85dada1aec24"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"def data_creation(X,Y,token):\n    input_ids=[]\n    attention_masks=[]\n    for sent in tqdm(X):\n        dbert_inps=token.encode_plus(str(sent),add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n        input_ids.append(dbert_inps['input_ids'])\n        attention_masks.append(dbert_inps['attention_mask'])\n\n    input_ids=np.asarray(input_ids)\n    attention_masks=np.array(attention_masks)\n    Y = np.array(Y)\n    print(Y.shape)\n    print(len(input_ids),len(attention_masks),len(Y))\n    return input_ids,attention_masks,Y","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:53.798123Z","iopub.execute_input":"2021-08-28T07:03:53.798581Z","iopub.status.idle":"2021-08-28T07:03:53.805437Z","shell.execute_reply.started":"2021-08-28T07:03:53.798542Z","shell.execute_reply":"2021-08-28T07:03:53.804567Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_use = bert_model\ntoken_use = bert_tokenizer\ninput_ids,attention_masks,Y = data_creation(X,Y,token_use)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:03:53.807015Z","iopub.execute_input":"2021-08-28T07:03:53.807530Z","iopub.status.idle":"2021-08-28T07:04:19.230336Z","shell.execute_reply.started":"2021-08-28T07:03:53.807493Z","shell.execute_reply":"2021-08-28T07:04:19.229288Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"864d8d44282942598f8c29c0777f39c3"}},"metadata":{}},{"name":"stdout","text":"(24000,)\n24000 24000 24000\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM,add\nfrom tensorflow.keras.layers import Flatten , Conv1D\nimport tensorflow as tf\ntf.config.run_functions_eagerly(True)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten , Conv1D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D,MaxPooling1D\nfrom tensorflow.keras.utils import plot_model\ndef create_model(max_len,num_classes,mod):\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    dbert_layer = mod(inps, attention_mask=masks)[0][:,0,:]\n    dense = Dense(4096,activation='relu')(dbert_layer)\n    dense = Reshape(( 256 , 16))(dense)\n    head4 = Conv1D(36,5)(dense\n                        )\n    head4 = MaxPooling1D(5,padding='same')(head4)\n    head4 = Flatten()(head4)\n    dropout= Dropout(0.2)(head4)\n    pred = Dense(1024, activation='softmax')(dropout)\n    pred = Dropout(0.2)(pred)\n    pred = Dense(64, activation='softmax')(pred)\n    pred = Dropout(0.5)(pred)\n    pred = Dense(32, activation='softmax')(pred)\n    pred = Dropout(0.5)(pred)\n    pred = Dense(num_classes, activation='softmax')(pred)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:31.116523Z","iopub.execute_input":"2021-08-28T07:04:31.116855Z","iopub.status.idle":"2021-08-28T07:04:31.128227Z","shell.execute_reply.started":"2021-08-28T07:04:31.116823Z","shell.execute_reply":"2021-08-28T07:04:31.127344Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = create_model(max_len,num_classes,model_use)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:33.515141Z","iopub.execute_input":"2021-08-28T07:04:33.515470Z","iopub.status.idle":"2021-08-28T07:04:35.233385Z","shell.execute_reply.started":"2021-08-28T07:04:33.515439Z","shell.execute_reply":"2021-08-28T07:04:35.232439Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x7f8c965c0d90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x7f8c965c0d90>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f8c965c0a10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f8c965c0a10>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f8c95a71f50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f8c95a71f50>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f8c95abdd90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f8c95abdd90>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f8c96106290>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f8c96106290>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f8c961063d0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f8c961063d0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f8c95abd2d0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f8c95abd2d0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfOutput object at 0x7f8c96289610>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertSelfOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfOutput object at 0x7f8c96289610>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.models.bert.modeling_tf_bert.TFBertIntermediate object at 0x7f8c96289e90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertIntermediate.call of <transformers.models.bert.modeling_tf_bert.TFBertIntermediate object at 0x7f8c96289e90>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertOutput object at 0x7f8c9653c650>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertOutput object at 0x7f8c9653c650>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f8c95abde10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: Unable to locate the source code of <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f8c95abde10>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 30)]         0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 30)]         0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 4096)         3149824     tf.__operators__.getitem[0][0]   \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 256, 16)      0           dense[0][0]                      \n__________________________________________________________________________________________________\nconv1d (Conv1D)                 (None, 252, 36)      2916        reshape[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling1d (MaxPooling1D)    (None, 51, 36)       0           conv1d[0][0]                     \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 1836)         0           max_pooling1d[0][0]              \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 1836)         0           flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1024)         1881088     dropout_37[0][0]                 \n__________________________________________________________________________________________________\ndropout_38 (Dropout)            (None, 1024)         0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 64)           65600       dropout_38[0][0]                 \n__________________________________________________________________________________________________\ndropout_39 (Dropout)            (None, 64)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 32)           2080        dropout_39[0][0]                 \n__________________________________________________________________________________________________\ndropout_40 (Dropout)            (None, 32)           0           dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 6)            198         dropout_40[0][0]                 \n==================================================================================================\nTotal params: 114,583,946\nTrainable params: 114,583,946\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:36:02.970835Z","iopub.execute_input":"2021-08-28T05:36:02.97121Z","iopub.status.idle":"2021-08-28T05:36:03.672924Z","shell.execute_reply.started":"2021-08-28T05:36:02.971171Z","shell.execute_reply":"2021-08-28T05:36:03.671738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Preparing the pickle file.....')\n\npickle_inp_path='dbert_inp.pkl'\npickle_mask_path='dbert_mask.pkl'\npickle_Pred_path='dbert_Pred.pkl'\n\npickle.dump((input_ids),open(pickle_inp_path,'wb'))\npickle.dump((attention_masks),open(pickle_mask_path,'wb'))\npickle.dump((Y),open(pickle_Pred_path,'wb'))\n\n\nprint('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_Pred_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:37.821927Z","iopub.execute_input":"2021-08-28T07:04:37.822261Z","iopub.status.idle":"2021-08-28T07:04:37.840925Z","shell.execute_reply.started":"2021-08-28T07:04:37.822230Z","shell.execute_reply":"2021-08-28T07:04:37.839916Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Preparing the pickle file.....\nPickle files saved as  dbert_inp.pkl dbert_mask.pkl dbert_Pred.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Loading the saved pickle files..')\n\ninput_ids=pickle.load(open(pickle_inp_path, 'rb'))\nattention_masks=pickle.load(open(pickle_mask_path, 'rb'))\nPreds=pickle.load(open(pickle_Pred_path, 'rb'))\n\nprint('Input shape {} Attention mask shape {} Input Pred shape {}'.format(input_ids.shape,attention_masks.shape,Preds.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:38.767320Z","iopub.execute_input":"2021-08-28T07:04:38.770057Z","iopub.status.idle":"2021-08-28T07:04:38.788710Z","shell.execute_reply.started":"2021-08-28T07:04:38.770011Z","shell.execute_reply":"2021-08-28T07:04:38.787433Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Loading the saved pickle files..\nInput shape (24000, 30) Attention mask shape (24000, 30) Input Pred shape (24000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"num_classes","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:40.625157Z","iopub.execute_input":"2021-08-28T07:04:40.625480Z","iopub.status.idle":"2021-08-28T07:04:40.631342Z","shell.execute_reply.started":"2021-08-28T07:04:40.625449Z","shell.execute_reply":"2021-08-28T07:04:40.630209Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"train_inp,test_inp,train_Pred,test_Pred,train_mask,test_mask=train_test_split(input_ids,Preds,attention_masks,test_size=0.2)\n\n\n\nlog_dir='dbert_model'\nmodel_save_path='./dbert_model.h5'\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\nmodel.compile(loss=loss,optimizer=optimizer, metrics=[metric])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:42.284612Z","iopub.execute_input":"2021-08-28T07:04:42.284968Z","iopub.status.idle":"2021-08-28T07:04:42.319231Z","shell.execute_reply.started":"2021-08-28T07:04:42.284933Z","shell.execute_reply":"2021-08-28T07:04:42.318451Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print('Train inp shape {} Test input shape {}\\nTrain Pred shape {} Test Pred shape {}\\nTrain attention mask shape {} Test attention mask shape {}'.format(train_inp.shape,test_inp.shape,train_Pred.shape,test_Pred.shape,train_mask.shape,test_mask.shape))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:43.472849Z","iopub.execute_input":"2021-08-28T07:04:43.473177Z","iopub.status.idle":"2021-08-28T07:04:43.478841Z","shell.execute_reply.started":"2021-08-28T07:04:43.473146Z","shell.execute_reply":"2021-08-28T07:04:43.477595Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Train inp shape (19200, 30) Test input shape (4800, 30)\nTrain Pred shape (19200,) Test Pred shape (4800,)\nTrain attention mask shape (19200, 30) Test attention mask shape (4800, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"history=model.fit([train_inp,train_mask],train_Pred,batch_size=64,epochs=10,validation_data=([test_inp,test_mask],test_Pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:45.653938Z","iopub.execute_input":"2021-08-28T07:04:45.654281Z","iopub.status.idle":"2021-08-28T07:23:20.445813Z","shell.execute_reply.started":"2021-08-28T07:04:45.654249Z","shell.execute_reply":"2021-08-28T07:23:20.444994Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/10\n300/300 [==============================] - 117s 371ms/step - loss: 1.7933 - accuracy: 0.1684 - val_loss: 1.7924 - val_accuracy: 0.1656\nEpoch 2/10\n300/300 [==============================] - 112s 373ms/step - loss: 1.7931 - accuracy: 0.1684 - val_loss: 1.7923 - val_accuracy: 0.1656\nEpoch 3/10\n300/300 [==============================] - 111s 372ms/step - loss: 1.7929 - accuracy: 0.1673 - val_loss: 1.7923 - val_accuracy: 0.1656\nEpoch 4/10\n300/300 [==============================] - 111s 370ms/step - loss: 1.7927 - accuracy: 0.1716 - val_loss: 1.7922 - val_accuracy: 0.1656\nEpoch 5/10\n300/300 [==============================] - 111s 370ms/step - loss: 1.7924 - accuracy: 0.1718 - val_loss: 1.7922 - val_accuracy: 0.1656\nEpoch 6/10\n300/300 [==============================] - 111s 369ms/step - loss: 1.7928 - accuracy: 0.1677 - val_loss: 1.7921 - val_accuracy: 0.1656\nEpoch 7/10\n300/300 [==============================] - 111s 369ms/step - loss: 1.7923 - accuracy: 0.1653 - val_loss: 1.7921 - val_accuracy: 0.1656\nEpoch 8/10\n300/300 [==============================] - 111s 368ms/step - loss: 1.7931 - accuracy: 0.1685 - val_loss: 1.7921 - val_accuracy: 0.1656\nEpoch 9/10\n300/300 [==============================] - 111s 370ms/step - loss: 1.7922 - accuracy: 0.1655 - val_loss: 1.7920 - val_accuracy: 0.1656\nEpoch 10/10\n300/300 [==============================] - 110s 367ms/step - loss: 1.7921 - accuracy: 0.1667 - val_loss: 1.7920 - val_accuracy: 0.1656\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"preds = model.predict([test_inp,test_mask],batch_size=16)\npred_Preds = preds.argmax(axis=1)\ntarget_names=le.classes_\nprint('Classification Report')\nprint(classification_report(test_Pred,pred_Preds,target_names=target_names))\nprint('Training and saving built model.....')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:23:20.447454Z","iopub.execute_input":"2021-08-28T07:23:20.447912Z","iopub.status.idle":"2021-08-28T07:23:46.849229Z","shell.execute_reply.started":"2021-08-28T07:23:20.447867Z","shell.execute_reply":"2021-08-28T07:23:46.848203Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Classification Report\n              precision    recall  f1-score   support\n\n barely-true       0.00      0.00      0.00       790\n       false       0.17      1.00      0.28       795\n   half-true       0.00      0.00      0.00       802\n mostly-true       0.00      0.00      0.00       801\n  pants-fire       0.00      0.00      0.00       800\n        true       0.00      0.00      0.00       812\n\n    accuracy                           0.17      4800\n   macro avg       0.03      0.17      0.05      4800\nweighted avg       0.03      0.17      0.05      4800\n\nTraining and saving built model.....\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create Test from test data and remove non corpus words","metadata":{}},{"cell_type":"code","source":"col_list=['Loc','Pred','Sent','Topic','Speaker','Position','Region','Party','a','b','c','d','e','Occasion']\ndfT = pd.read_csv('../input/newsdatasets/other_dataset/other_dataset/LIAR/test.tsv',header = None,sep='\\t')\ndfT.columns =  col_list\ndfT.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:23:57.065062Z","iopub.execute_input":"2021-08-28T07:23:57.065386Z","iopub.status.idle":"2021-08-28T07:23:57.137546Z","shell.execute_reply.started":"2021-08-28T07:23:57.065355Z","shell.execute_reply":"2021-08-28T07:23:57.136566Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"          Loc        Pred                                               Sent  \\\n0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n1  11685.json       false  Wisconsin is on pace to double the number of l...   \n2  11096.json       false  Says John McCain has done nothing to help the ...   \n3   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n4   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n\n                                               Topic  \\\n0                                        immigration   \n1                                               jobs   \n2                    military,veterans,voting-record   \n3  medicare,message-machine-2012,campaign-adverti...   \n4  campaign-finance,legal-issues,campaign-adverti...   \n\n                            Speaker              Position     Region  \\\n0                        rick-perry              Governor      Texas   \n1                 katrina-shankland  State representative  Wisconsin   \n2                      donald-trump       President-Elect   New York   \n3                     rob-cornilles            consultant     Oregon   \n4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n\n        Party   a    b   c   d   e                      Occasion  \n0  republican  30   30  42  23  18               Radio interview  \n1    democrat   2    1   0   0   0             a news conference  \n2  republican  63  114  51  37  61  comments on ABC's This Week.  \n3  republican   1    1   3   1   1                  a radio show  \n4    democrat   5    7   2   2   7                   a web video  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loc</th>\n      <th>Pred</th>\n      <th>Sent</th>\n      <th>Topic</th>\n      <th>Speaker</th>\n      <th>Position</th>\n      <th>Region</th>\n      <th>Party</th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n      <th>Occasion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11972.json</td>\n      <td>true</td>\n      <td>Building a wall on the U.S.-Mexico border will...</td>\n      <td>immigration</td>\n      <td>rick-perry</td>\n      <td>Governor</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>30</td>\n      <td>30</td>\n      <td>42</td>\n      <td>23</td>\n      <td>18</td>\n      <td>Radio interview</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11685.json</td>\n      <td>false</td>\n      <td>Wisconsin is on pace to double the number of l...</td>\n      <td>jobs</td>\n      <td>katrina-shankland</td>\n      <td>State representative</td>\n      <td>Wisconsin</td>\n      <td>democrat</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>a news conference</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11096.json</td>\n      <td>false</td>\n      <td>Says John McCain has done nothing to help the ...</td>\n      <td>military,veterans,voting-record</td>\n      <td>donald-trump</td>\n      <td>President-Elect</td>\n      <td>New York</td>\n      <td>republican</td>\n      <td>63</td>\n      <td>114</td>\n      <td>51</td>\n      <td>37</td>\n      <td>61</td>\n      <td>comments on ABC's This Week.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5209.json</td>\n      <td>half-true</td>\n      <td>Suzanne Bonamici supports a plan that will cut...</td>\n      <td>medicare,message-machine-2012,campaign-adverti...</td>\n      <td>rob-cornilles</td>\n      <td>consultant</td>\n      <td>Oregon</td>\n      <td>republican</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>a radio show</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9524.json</td>\n      <td>pants-fire</td>\n      <td>When asked by a reporter whether hes at the ce...</td>\n      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n      <td>state-democratic-party-wisconsin</td>\n      <td>NaN</td>\n      <td>Wisconsin</td>\n      <td>democrat</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>a web video</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"mask = (dfT['Sent'].str.len() < 150) \ndfT = dfT.loc[mask]\ndfT.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:23:59.225536Z","iopub.execute_input":"2021-08-28T07:23:59.225930Z","iopub.status.idle":"2021-08-28T07:23:59.251051Z","shell.execute_reply.started":"2021-08-28T07:23:59.225895Z","shell.execute_reply":"2021-08-28T07:23:59.250281Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(1057, 14)"},"metadata":{}}]},{"cell_type":"code","source":"Xt = dfT['Sent']\nYt = dfT['Pred']\nYt = le.transform(Yt)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:24:00.649299Z","iopub.execute_input":"2021-08-28T07:24:00.649614Z","iopub.status.idle":"2021-08-28T07:24:00.656903Z","shell.execute_reply.started":"2021-08-28T07:24:00.649584Z","shell.execute_reply":"2021-08-28T07:24:00.656114Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dfT.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:24:04.090123Z","iopub.execute_input":"2021-08-28T07:24:04.090449Z","iopub.status.idle":"2021-08-28T07:24:04.096403Z","shell.execute_reply.started":"2021-08-28T07:24:04.090418Z","shell.execute_reply":"2021-08-28T07:24:04.095442Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(1057, 14)"},"metadata":{}}]},{"cell_type":"code","source":"input_idsT,attention_masksT,YT = data_creation(Xt,Yt,token_use)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:24:06.705339Z","iopub.execute_input":"2021-08-28T07:24:06.705681Z","iopub.status.idle":"2021-08-28T07:24:07.439782Z","shell.execute_reply.started":"2021-08-28T07:24:06.705649Z","shell.execute_reply":"2021-08-28T07:24:07.438805Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f734978757146a59f3d9a53bc510285"}},"metadata":{}},{"name":"stdout","text":"(1057,)\n1057 1057 1057\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = model.predict([input_idsT,attention_masksT],batch_size=16)\npred_Preds = preds.argmax(axis=1)\ntarget_names=le.classes_\n#$print('F1 score',f1)\nprint('Classification Report')\nprint(classification_report(Yt,pred_Preds,target_names=target_names))\nprint('Training and saving built model.....')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:24:09.383601Z","iopub.execute_input":"2021-08-28T07:24:09.383959Z","iopub.status.idle":"2021-08-28T07:24:15.127381Z","shell.execute_reply.started":"2021-08-28T07:24:09.383926Z","shell.execute_reply":"2021-08-28T07:24:15.126482Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Classification Report\n              precision    recall  f1-score   support\n\n barely-true       0.00      0.00      0.00       177\n       false       0.21      1.00      0.35       221\n   half-true       0.00      0.00      0.00       216\n mostly-true       0.00      0.00      0.00       192\n  pants-fire       0.00      0.00      0.00        72\n        true       0.00      0.00      0.00       179\n\n    accuracy                           0.21      1057\n   macro avg       0.03      0.17      0.06      1057\nweighted avg       0.04      0.21      0.07      1057\n\nTraining and saving built model.....\n","output_type":"stream"}]}]}