{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('../FakeNewsNet/Preprocessed_FakeNewsNet.csv', quotechar='\"', index_col=0, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nvidia_smi\n",
    "\n",
    "# nvidia_smi.nvmlInit()\n",
    "\n",
    "# deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "# for i in range(deviceCount):\n",
    "#     handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "#     info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "\n",
    "# nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all LIAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_titles = ['statement_ID', 'label', 'statement', 'subject', 'speaker', 'speaker_job_title', 'state_info','party_affiliation', 'barely_true',\n",
    "                  'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
    "\n",
    "file_dir = '../dataset/liar_dataset_/'\n",
    "\n",
    "def dataset_development(dir_, new_title):\n",
    "    dataset = pd.read_csv(dir_, sep = '\\t', header=None, index_col = False)\n",
    "#     titles = ['col_' + str(i) for i in range(1, len(list(dataset.columns))+1)]\n",
    "    titles = [i for i in range(len(list(dataset.columns)))]\n",
    "    title_dict = {i:j for i,j in zip(titles, new_title)}\n",
    "    dataset = dataset.rename(columns=title_dict)\n",
    "#     dataset.columns = new_title\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_data = dataset_development(file_dir + 'train.tsv', original_titles)\n",
    "valid_data = dataset_development(file_dir + 'valid.tsv', original_titles)\n",
    "test_data = dataset_development(file_dir + 'test.tsv', original_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['y'] = train_data['label'].apply(lambda x: 0 if x == 'pants-fire' else 1 if x == 'false'\n",
    "                                         else 2 if x == 'half-true' else 3 if x == 'mostly-true' else 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data['y'] = valid_data['label'].apply(lambda x: 0 if x == 'pants-fire' else 1 if x == 'false'\n",
    "                                         else 2 if x == 'half-true' else 3 if x == 'mostly-true' else 4)\n",
    "test_data['y'] = test_data['label'].apply(lambda x: 0 if x == 'pants-fire' else 1 if x == 'false'\n",
    "                                         else 2 if x == 'half-true' else 3 if x == 'mostly-true' else 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 15)\n",
      "(1284, 15)\n",
      "(1267, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    all_text_contents = list()\n",
    "#     lines = df[\"text\"].values.tolist()\n",
    "    lines = df[\"statement\"].astype('string').tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        \n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        \n",
    "        emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        text = emoji.sub(r'', text)\n",
    "        \n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"i've\", \"i have\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)        \n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"they're\", \"they are\", text)        \n",
    "        text = re.sub(r\"you're\", \"you are\", text)   \n",
    "        text = re.sub(r\"you've\", \"you have\", text)   \n",
    "        text = re.sub(r\"we've\", \"we have\", text) \n",
    "        text = re.sub(r\"they've\", \"they have\", text) \n",
    "#         text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text) \n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"have't\", \"have not\", text)\n",
    "        \n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \" \", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         stop_words.discard(\"not\")\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_text_contents.append(words)\n",
    "    return all_text_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizing_and_more(corpus):\n",
    "#     texts = []\n",
    "#     for text in corpus:\n",
    "# #         lemmatized_words = []\n",
    "#         tokenized_words = []\n",
    "# #         for word in text.split(\" \"):\n",
    "#         for line in text:\n",
    "# #             if word not in set(stopwords.words('english')):\n",
    "# #                 lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "#             tokenized_words = \n",
    "#             tokenized_words.append(word)\n",
    "#         texts.append(tokenized_words)\n",
    "            \n",
    "#     return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleansed_statements = clean_text(train_data)\n",
    "len(train_cleansed_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['preprocessed_statement'] = train_cleansed_statements\n",
    "train_data['tokenized_preprocessed_statement'] = train_data['preprocessed_statement'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['word_count'] = train_data['tokenized_preprocessed_statement'].apply(lambda x: len(x))\n",
    "train_data['word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job_title</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true</th>\n",
       "      <th>false</th>\n",
       "      <th>half_true</th>\n",
       "      <th>mostly_true</th>\n",
       "      <th>pants_on_fire</th>\n",
       "      <th>context</th>\n",
       "      <th>y</th>\n",
       "      <th>preprocessed_statement</th>\n",
       "      <th>tokenized_preprocessed_statement</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>1</td>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "      <td>[says, the, annies, list, political, group, su...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>2</td>\n",
       "      <td>when did the decline of coal start it started ...</td>\n",
       "      <td>[when, did, the, decline, of, coal, start, it,...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>3</td>\n",
       "      <td>hillary clinton agrees with john mccain by vot...</td>\n",
       "      <td>[hillary, clinton, agrees, with, john, mccain,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>1</td>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "      <td>[health, care, reform, legislation, is, likely...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>2</td>\n",
       "      <td>the economic turnaround started at the end of ...</td>\n",
       "      <td>[the, economic, turnaround, started, at, the, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  statement_ID        label  \\\n",
       "0    2635.json        false   \n",
       "1   10540.json    half-true   \n",
       "2     324.json  mostly-true   \n",
       "3    1123.json        false   \n",
       "4    9028.json    half-true   \n",
       "\n",
       "                                           statement  \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3  Health care reform legislation is likely to ma...   \n",
       "4  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker     speaker_job_title  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "  state_info party_affiliation  barely_true  false  half_true  mostly_true  \\\n",
       "0      Texas        republican          0.0    1.0        0.0          0.0   \n",
       "1   Virginia          democrat          0.0    0.0        1.0          1.0   \n",
       "2   Illinois          democrat         70.0   71.0      160.0        163.0   \n",
       "3        NaN              none          7.0   19.0        3.0          5.0   \n",
       "4    Florida          democrat         15.0    9.0       20.0         19.0   \n",
       "\n",
       "   pants_on_fire              context  y  \\\n",
       "0            0.0             a mailer  1   \n",
       "1            0.0      a floor speech.  2   \n",
       "2            9.0               Denver  3   \n",
       "3           44.0       a news release  1   \n",
       "4            2.0  an interview on CNN  2   \n",
       "\n",
       "                              preprocessed_statement  \\\n",
       "0  says the annies list political group supports ...   \n",
       "1  when did the decline of coal start it started ...   \n",
       "2  hillary clinton agrees with john mccain by vot...   \n",
       "3  health care reform legislation is likely to ma...   \n",
       "4  the economic turnaround started at the end of ...   \n",
       "\n",
       "                    tokenized_preprocessed_statement  word_count  \n",
       "0  [says, the, annies, list, political, group, su...          12  \n",
       "1  [when, did, the, decline, of, coal, start, it,...          24  \n",
       "2  [hillary, clinton, agrees, with, john, mccain,...          19  \n",
       "3  [health, care, reform, legislation, is, likely...          12  \n",
       "4  [the, economic, turnaround, started, at, the, ...          10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv('C:/Users/Jiwi/Documents/NLP/Fakes News Detection/dataset/liar_dataset_/preprocessed_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for long texts: https://github.com/jamescalam/transformers/blob/main/course/language_classification/04_window_method_in_pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Take a look at flair's  TransformerWordEmbeddings!!!! '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary with which BERT was pre-trained on: 28996\n"
     ]
    }
   ],
   "source": [
    "# list(tokenizer.vocab.keys())[5000:5020]\n",
    "model = BertModel.from_pretrained('bert-base-cased', output_hidden_states = True).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Size of Vocabulary with which BERT was pre-trained on:\", len(list(tokenizer.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ = '[CLS] ' +  train_cleansed_statements[0] + ' [SEP]'\n",
    "# tokenized_test = tokenizer.tokenize(input_)\n",
    "# tokenized_test\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_test)\n",
    "# indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_test\n",
    "\n",
    "# segments_ids = [1] * len(tokenized_test)\n",
    "# print('segments_ids length:', len(segments_ids))\n",
    "# print('tokenized text length:', len(tokenized_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ------------- Some important notes in BERT embeddings ------------- '''\n",
    "\n",
    "'''hash signs preceding some of these subwords are just our tokenizer’s way to denote that this subword \n",
    "or character is part of a larger word and preceded by another subword.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data for bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# text = \"Replace me by any text you'd like because I hate dirty space here.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt', do_lower_case =True)  \n",
    "# encoded_input\n",
    "\n",
    "def bert_tokenized_text(corpus):\n",
    "    embedding_info = dict()\n",
    "    for idx, corp in enumerate(corpus):\n",
    "        input_ = '[CLS] ' +  corp + '.' + ' [SEP]'\n",
    "        tokenized_texts = tokenizer.tokenize(input_)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_texts)\n",
    "        segments_ids = [idx] * len(tokenized_texts)\n",
    "        \n",
    "        assert len(tokenized_texts) == len(segments_ids)\n",
    "        assert len(tokenized_texts) == len(indexed_tokens)\n",
    "        \n",
    "        embedding_info['Sentence_' + str(idx)] = {'Tokenized corpus': tokenized_texts, \n",
    "                                                 'Indexed Tokens': indexed_tokens, 'Segment IDs': segments_ids}\n",
    "    return embedding_info\n",
    "        \n",
    "Embedding_input_info_liar_train = bert_tokenized_text(train_cleansed_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sentence_0', {'Tokenized corpus': ['[CLS]', 'says', 'the', 'an', '##nies', 'list', 'political', 'group', 'supports', 'third', 'trim', '##ester', 'abortion', '##s', 'on', 'demand', '.', '[SEP]'], 'Indexed Tokens': [101, 1867, 1103, 1126, 16133, 2190, 1741, 1372, 6253, 1503, 13373, 12831, 12030, 1116, 1113, 4555, 119, 102], 'Segment IDs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})"
     ]
    }
   ],
   "source": [
    "print(list(Embedding_input_info_liar_train.items())[0], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find highest number of size of indexed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_id_sizes = []\n",
    "for tups in list(Embedding_input_info_liar_train.items()):\n",
    "    sentence_info = tups[1]\n",
    "    indexed_tokens_size = len(sentence_info['Indexed Tokens'])\n",
    "    sentence_token_id_sizes.append(indexed_tokens_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Size of Sentence Token ID: 596\n"
     ]
    }
   ],
   "source": [
    "print('Highest Size of Sentence Token ID:', max(sentence_token_id_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(Embedding_input_info_liar_train.items())[:1][0][1], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_tensor = torch.Tensor([list(Embedding_input_info_liar_train.items())[:1][0][1]['Indexed Tokens']]).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained('bert-base-cased', output_hidden_states = True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample bert testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Embedding_input_info_liar_train, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_embedding = Embedding_input_info_liar_train['Sentence_0']['Indexed Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_tensor = torch.zeros(10).reshape(1, -1).to(device)\n",
    "# zero_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_tensor.shape\n",
    "# zero_tensor = torch.zeros(10).reshape(1, -1).to(device)\n",
    "# token_tensor_padded = torch.cat([token_tensor, zero_tensor], dim=1).to(device)\n",
    "# token_tensor_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_tensor_padded = token_tensor_padded.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_tensor_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # token_tensor = torch.LongTensor([first_embedding]).cuda(device)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(token_tensor_padded.cuda(device))\n",
    "#     hidden_states = outputs[2]\n",
    "#     token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "#     token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "# #         print(token_embeddings.size())\n",
    "#     token_padded_embeddings = token_embeddings.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_padded_embeddings[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_tensor = torch.zeros(10).reshape(1, -1).type(torch.LongTensor).to(device)\n",
    "# token_tensor_padded = torch.cat([token_tensor, zero_tensor], dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                              | 6191/10240 [56:15<36:47,  1.83it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.00 GiB total capacity; 9.52 GiB already allocated; 19.75 MiB free; 9.57 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-64abb2f189b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-64abb2f189b3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mlayer_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_bert_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding_input_info_liar_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mtoken_stacked_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massemble_and_bert_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../dataset/liar_dataset_/huggingface_bert_embeddings/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0msave_bert_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_stacked_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-64abb2f189b3>\u001b[0m in \u001b[0;36mcreate_bert_embeddings\u001b[1;34m(input_dictionary)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtoken_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mtoken_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m#         print(token_embeddings.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.00 GiB total capacity; 9.52 GiB already allocated; 19.75 MiB free; 9.57 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "def create_bert_embeddings(input_dictionary):\n",
    "#     model.eval()\n",
    "    max_seq_len = 512\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print('Device:', device)\n",
    "#     current_device = device\n",
    "    # sep_indicator = torch.LongTensor([102]).cuda(device)\n",
    "    sep_indicator = torch.LongTensor([102]).to(device)\n",
    "    layer_embeddings = []\n",
    "\n",
    "    for idx, tups in enumerate(tqdm(list(input_dictionary.items()))):\n",
    "#         token_ids, segment_ids = tups[1]['Indexed Tokens'], tups[1]['Segment IDs']\n",
    "        token_ids = tups[1]['Indexed Tokens']\n",
    "    #     model.eval()\n",
    "#         tokens_tensor, segment_id_tensor = torch.LongTensor([token_ids]).to(device), torch.Tensor([segment_ids]).to(device)\n",
    "        tokens_tensor = torch.LongTensor([token_ids]).to(device)\n",
    "#         print('tokens_tensor:', tokens_tensor.is_cuda)\n",
    "        if tokens_tensor.shape[1] > max_seq_len:\n",
    "\n",
    "            tokens_tensor = tokens_tensor[:,:max_seq_len-1].to(device)\n",
    "#             print('tokens_tensor 1:', tokens_tensor.is_cuda)\n",
    "            tokens_tensor = torch.cat((tokens_tensor, sep_indicator.unsqueeze(0)), dim=-1).to(device)\n",
    "#             print('tokens_tensor 2:', tokens_tensor.is_cuda)\n",
    "\n",
    "        else:\n",
    "            zero_tensor = torch.zeros(max_seq_len-tokens_tensor.shape[1]).reshape(1, -1).to(device)\n",
    "#             print('zero_tensor :', zero_tensor.is_cuda)\n",
    "            tokens_tensor = torch.cat([tokens_tensor, zero_tensor], dim=1).type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "    #         print(tokens_tensor, tokens_tensor.shape)\n",
    "#         print('tokens_tensor:', tokens_tensor.is_cuda)\n",
    "    #     print('tensor type:', type(tokens_tensor))\n",
    "\n",
    "        # one of these tokens tensors has size: torch.Size([1, 596])\n",
    "\n",
    "        # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "#         print('current device:', device)\n",
    "        with torch.no_grad():\n",
    "            # the third item will be the hidden states from all layers.\n",
    "            \n",
    "            outputs = model(tokens_tensor)\n",
    "            \n",
    "            \n",
    "            tokens_tensor.to('cpu')\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "            token_embeddings = torch.squeeze(token_embeddings, dim=1).permute(1,0,2)\n",
    "    #         print(token_embeddings.size())\n",
    "#             token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "            # sum of the outputs from last 4 layers\n",
    "            summed_tok_vecs = []\n",
    "\n",
    "            for token in token_embeddings:\n",
    "    #             print('embeddings from last 4 layers:', token[-4:])\n",
    "    #             print('=' * 100)\n",
    "#                 summed_tok_vecs.append(torch.sum(token[-4:], dim=0))\n",
    "#                 sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    #             print('summed vector:',  sum_vec)\n",
    "    #             print('shape of summed vector:', sum_vec.shape)\n",
    "#                 summed_tok_vecs.append(sum_vec)\n",
    "                summed_tok_vecs.append(torch.sum(token[-4:], dim=0))\n",
    "\n",
    "\n",
    "            layer_embeddings.append(summed_tok_vecs)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    return layer_embeddings\n",
    "\n",
    "def assemble_and_bert_embeddings(layer_embeddings, destination_dir):\n",
    "    stacked_vectors_ = []\n",
    "    for i in range(len(layer_embeddings)):\n",
    "        vecs_to_stack = tuple([embed_vect for embed_vect in layer_embeddings[i]])\n",
    "\n",
    "        stacked_vectors = torch.stack(vecs_to_stack, dim=0)\n",
    "        stacked_vectors_.append(stacked_vectors)\n",
    "        \n",
    "#     with open(destination_dir + 'huggingface_bert_train_embeddings.pickle', 'wb') as f:\n",
    "#         pickle.dump(stacked_vectors_, f)\n",
    "        \n",
    "    return stacked_vectors_\n",
    "    \n",
    "def save_bert_embeddings(stacked_vectors):\n",
    "    destination_dir = '../dataset/liar_dataset_/huggingface_bert_embeddings/'\n",
    "    with open(destination_dir + 'huggingface_bert_train_embeddings.pickle', 'wb') as f:\n",
    "        pickle.dump(stacked_vectors, f)\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    layer_embeddings = create_bert_embeddings(Embedding_input_info_liar_train)\n",
    "    token_stacked_embeddings = assemble_and_bert_embeddings(layer_embeddings, '../dataset/liar_dataset_/huggingface_bert_embeddings/')\n",
    "    save_bert_embeddings(token_stacked_embeddings)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_dir = '../dataset/liar_dataset_/huggingface_bert_embeddings/'\n",
    "with open(destination_dir + 'huggingface_bert_train_embeddings.pickle', 'rb') as pickle_file:\n",
    "    loaded_train_embeddings = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([512, 768]) torch.Size([512, 768])\n",
      "torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(loaded_train_embeddings))\n",
    "print(loaded_train_embeddings[0].shape, loaded_train_embeddings[1].shape)\n",
    "print(loaded_train_embeddings[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_file = open(destination_dir + 'number.pickle', 'rb')\n",
    "# data = pickle.load(pkl_file)\n",
    "# print(data)\n",
    "# print(type(data))\n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all embeddings together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_vectors_ = []\n",
    "# for i in range(len(sample_embedding)):\n",
    "#     vecs_to_stack = tuple([embed_vect for embed_vect in sample_embedding[i]])\n",
    "\n",
    "#     stacked_vectors = torch.stack(vecs_to_stack, dim=0)\n",
    "#     stacked_vectors_.append(stacked_vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(stacked_vectors_))\n",
    "# print(stacked_vectors_[0].shape, stacked_vectors_[1].shape)\n",
    "# print(stacked_vectors_[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/liar_dataset_/huggingface_bert_embeddings/huggingface_bert_train_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(stacked_vectors_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair's Embedding package (another option for embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_train_data = pd.read_csv('../dataset/liar_dataset_/preprocessed_train.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job_title</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true</th>\n",
       "      <th>false</th>\n",
       "      <th>half_true</th>\n",
       "      <th>mostly_true</th>\n",
       "      <th>pants_on_fire</th>\n",
       "      <th>context</th>\n",
       "      <th>preprocessed_statement</th>\n",
       "      <th>tokenized_preprocessed_statement</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "      <td>[says, the, annies, list, political, group, su...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>when did the decline of coal start it started ...</td>\n",
       "      <td>[when, did, the, decline, of, coal, start, it,...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>hillary clinton agrees with john mccain by vot...</td>\n",
       "      <td>[hillary, clinton, agrees, with, john, mccain,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "      <td>[health, care, reform, legislation, is, likely...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>the economic turnaround started at the end of ...</td>\n",
       "      <td>[the, economic, turnaround, started, at, the, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  statement_ID        label  \\\n",
       "0    2635.json        false   \n",
       "1   10540.json    half-true   \n",
       "2     324.json  mostly-true   \n",
       "3    1123.json        false   \n",
       "4    9028.json    half-true   \n",
       "\n",
       "                                           statement  \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3  Health care reform legislation is likely to ma...   \n",
       "4  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker     speaker_job_title  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "  state_info party_affiliation  barely_true  false  half_true  mostly_true  \\\n",
       "0      Texas        republican          0.0    1.0        0.0          0.0   \n",
       "1   Virginia          democrat          0.0    0.0        1.0          1.0   \n",
       "2   Illinois          democrat         70.0   71.0      160.0        163.0   \n",
       "3        NaN              none          7.0   19.0        3.0          5.0   \n",
       "4    Florida          democrat         15.0    9.0       20.0         19.0   \n",
       "\n",
       "   pants_on_fire              context  \\\n",
       "0            0.0             a mailer   \n",
       "1            0.0      a floor speech.   \n",
       "2            9.0               Denver   \n",
       "3           44.0       a news release   \n",
       "4            2.0  an interview on CNN   \n",
       "\n",
       "                              preprocessed_statement  \\\n",
       "0  says the annies list political group supports ...   \n",
       "1  when did the decline of coal start it started ...   \n",
       "2  hillary clinton agrees with john mccain by vot...   \n",
       "3  health care reform legislation is likely to ma...   \n",
       "4  the economic turnaround started at the end of ...   \n",
       "\n",
       "                    tokenized_preprocessed_statement  word_count  \n",
       "0  [says, the, annies, list, political, group, su...          12  \n",
       "1  [when, did, the, decline, of, coal, start, it,...          24  \n",
       "2  [hillary, clinton, agrees, with, john, mccain,...          19  \n",
       "3  [health, care, reform, legislation, is, likely...          12  \n",
       "4  [the, economic, turnaround, started, at, the, ...          10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['false', 'half-true', 'mostly-true', 'true', 'barely-true',\n",
       "       'pants-fire'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### give labels their numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_embedding = TransformerWordEmbeddings('bert-base-uncased', subtoken_pooling='mean', layers='-1,-2,-3,-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flair.data.Sentence"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_1 = Sentence(list(train_data['preprocessed_statement'])[0])\n",
    "type(sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"says the annies list political group supports third trimester abortions on demand\"   [− Tokens: 12]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [tok for tok in sentence_1]\n",
    "bert_embedding.embed(sentence_1)\n",
    "\n",
    "# print(sentence_1[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_1[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10240/10240 [1:29:57<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# bert_embedding.embed(sentence_1)\n",
    "corpus_embeddings = []\n",
    "for corpus in tqdm(list(train_data['preprocessed_statement'])):\n",
    "    corpus = Sentence(corpus)\n",
    "    bert_embedding.embed(corpus)\n",
    "    token_embeddings = []\n",
    "    for token in corpus:\n",
    "    #     print(token.embedding.shape)\n",
    "        token_embeddings.append(token.embedding.unsqueeze(0).cuda(device))\n",
    "\n",
    "    token_embeddings = torch.cat(token_embeddings, dim=0).cuda(device)\n",
    "    corpus_embeddings.append(token_embeddings)\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/liar_dataset_/bert_embeddings/bert_train_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### putting results from layers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_embedding[0]\n",
    "\n",
    "# token_vecs_cat = []\n",
    "\n",
    "# for token in sample_embedding[0]:\n",
    "    \n",
    "#     # `token` is a [12 x 768] tensor\n",
    "\n",
    "#     # Concatenate the vectors (that is, append them together) from the last \n",
    "#     # four layers.\n",
    "#     # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "# #     print(token[-1].shape)\n",
    "#     cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "#     # Use `cat_vec` to represent `token`.\n",
    "#     token_vecs_cat.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confirming outputs of everything\n",
    "# print('Number of layers:', len(sample_embedding[0]))  # 13 layers\n",
    "# print('Number of batches:',len(sample_embedding[0][0]))  # 1 sentence\n",
    "# print('Number of tokens:',len(sample_embedding[0][0][0]))  # 17 tokens\n",
    "# print('Number of hidden units:',len(sample_embedding[0][0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the data into batches (TO be done once all embeddings are made)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, 3\n",
    "          # (in_channel, out_channel, kernel size)  Input dims: (batch size, Num channels in, Height_in, Width_in)\n",
    "                                            # Output dims: (batch size, Num channels out, Height_out, Width_out)   could start with filter size of 3 & 5\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  # (kernel_size, stride)  Input dims: (batch size, Num channels in, Height_in, Width_in)   \n",
    "        self.emb_size = 768\n",
    "        self.conv1 = nn.Conv1d(self.emb_size, 50, kernel_size=2, stride=2)      # Input dims: (batch size, Num channels in, L_in)  Output dims: (batch size, Num channels out, L_out)\n",
    "        self.pool1 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(50, 20, kernel_size=2, stride=2)\n",
    "        self.pool2 = nn.MaxPool1d(2, stride=2)\n",
    "        self.fc1 = nn.Linear(20 * 32, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)  # 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # used for multi-class classification, nn.BCEWithLogitsLoss() is used when there are binary classes \n",
    "                                    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get embeddings and labels as tuples'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify dimensions of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_train_true_y = list(train_data['y'])[:5]\n",
    "# sample_train_true_y\n",
    "# zero_vector = [np.zeros(5) for _ in range(len(sample_train_true_y))]\n",
    "\n",
    "# sample_train_true_ys = []\n",
    "# for vector, idx in zip(zero_vector, sample_train_true_y):\n",
    "\n",
    "#     vector[idx] = 1 \n",
    "#     sample_train_true_ys.append(vector)\n",
    "# true_y\n",
    "# zero_vector[1] = 1\n",
    "# zero_vector\n",
    "sample_train_true_y = [np.array([i]) for i in list(train_data['y'])[:5]]\n",
    "# print(sample_train_true_y)\n",
    "train_dataset = [(x, y) for x, y in zip(loaded_train_embeddings, sample_train_true_y)]\n",
    "# train_dataset\n",
    "x, y = train_dataset[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in enumerate(range(20)):\n",
    "#     plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_loss_plotting(loss_value):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_y: 4 torch.Size([1])\n",
      "pred_y: 4 torch.Size([1])\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_batch_losses, val_batch_losses = [], []\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times (epochs)\n",
    "\n",
    "    running_epoch_loss = 0.0\n",
    "    \n",
    "    # Here we are looping over each batch \n",
    "    for i, data in enumerate(train_dataset[:1], 0):   # in my case, I made the trainData as a list of tuples of (embedding(shape:[512, 768]), label)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        net.train()     \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(0).permute(0, 2, 1)\n",
    "   \n",
    "#         print(inputs.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)   # In this case the input has shape: (batch size, Num channels in, Height_in, Width_in)   \n",
    "                                 # shape: torch.Size([1, 5])\n",
    "        \n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        softmax_outputs = F.softmax(outputs, dim=1) # shape: torch.Size([1, 5])\n",
    "        \n",
    "        pred_y = torch.argmax(sample_softmax, dim=1)\n",
    "#         print('softmax_outputs:', softmax_outputs, softmax_outputs.shape)\n",
    "#         print('pred_y:', pred_y.item(), pred_y.shape)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "  \n",
    "        train_loss = criterion(softmax_outputs, torch.LongTensor(labels))\n",
    "        \n",
    "#         train_losses.append(train_loss)\n",
    "        \n",
    "#         print('loss:', loss)\n",
    "        train_loss.backward()  # calculates gradients\n",
    "        optimizer.step()   # Updating the weights\n",
    "\n",
    "        # print statistics\n",
    "        running_epoch_loss += train_loss.item()\n",
    "        if i % 1 == 1:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_epoch_loss / 2000))\n",
    "            running_epoch_loss = 0.0\n",
    "    train_batch_losses.append(running_epoch_loss)\n",
    "            \n",
    "        # Do the same with Validation data:\n",
    "#     va\n",
    "    val_running_loss = 0.0\n",
    "    for i, data in enumerate(valid_dataset, 0):   # in my case, I made the trainData as a list of tuples of (embedding(shape:[512, 768]), label)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        net.eval()     \n",
    "        valid_inputs, valid_labels = data\n",
    "        valid_inputs = valid_inputs.unsqueeze(0).permute(0, 2, 1)\n",
    "        \n",
    "        val_outputs = net(inputs)\n",
    "        val_softmax_outputs = F.softmax(val_outputs, dim=1)\n",
    "        val_pred_y = torch.argmax(val_softmax_outputs, dim=1)\n",
    "        \n",
    "        val_loss = criterion(val_softmax_outputs, torch.LongTensor(valid_labels))\n",
    "        \n",
    "        val_running_loss += val_loss/item()\n",
    "        \n",
    "    val_batch_losses.append(val_running_loss)\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "sample_softmax = torch.Tensor([[0.1828, 0.2236, 0.1699, 0.1922, 0.2315]])\n",
    "s_s_argnax = torch.argmax(sample_softmax, dim=1)\n",
    "\n",
    "print(s_s_argnax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../dataset/liar_dataset_/BERT_CNN_weights/cnn_bert_1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../dataset/liar_dataset_/BERT_CNN_weights/cnn_bert_1.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load saved weights to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4252, -3.8400, -3.8484,  ..., -5.4332,  1.9479,  3.5162],\n",
       "         [ 0.1662, -7.5079,  2.8423,  ..., -3.7434,  6.5641,  2.5978],\n",
       "         [-2.9217, -5.5550, -1.3506,  ..., -1.5238,  1.6359,  2.7358],\n",
       "         ...,\n",
       "         [-0.3025, -1.8622,  0.0407,  ..., -9.4621,  3.7189, -0.0241],\n",
       "         [ 0.3599, -3.6430, -0.7354,  ..., -9.5790,  2.8197, -1.4477],\n",
       "         [-2.5329, -3.1569, -0.9558,  ..., -8.0567,  2.8855, -1.3816]]),\n",
       " array([1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''net.eval() '''  # used to evaluate or test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(saved_model, dataset):\n",
    "    pred_test_ys, true_test_ys = [], []\n",
    "    saved_model.eval()  # This means we are testing the model \n",
    "    with torch.no_grad():  # exclude gradient computations.\n",
    "        for i, data in enumerate(dataset, 0):\n",
    "            inputs, true_test_y = data\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            pred_classes = saved_model(inputs)\n",
    "            softmax_test_output = F.softmax(pred_classes)\n",
    "\n",
    "            pred_test_y = torch.argmax(softmax_test_output, dim=1)\n",
    "            pred_test_ys.append(pred_test_y.item())\n",
    "            true_test_ys.append(true_test_y)\n",
    "        \n",
    "    test_accuracy = accuracy_score(true_test_ys, pred_test_ys)\n",
    "    test_precision = precision_score(true_test_ys, pred_test_ys, average='micro')\n",
    "    test_f1 = f1_score(true_test_ys, pred_test_ys, average='micro')\n",
    "    \n",
    "    return test_accuracy, test_precision, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-9efa87b52775>:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_test_output = F.softmax(pred_classes)\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_precision, test_f1 = test_model(net, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
